{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://www.kaggle.com/code/risheshg/m5-accuracy-starter-data-exploration?scriptVersionId=97463246\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["# M5 Forecasting Challenge"]},{"cell_type":"markdown","metadata":{},"source":["Hey Guys!! This is my first Notebook I am submitting here on this platform. There may be many mistakes here but kindly bear with me. Any comments would be highly appreciated.\n","  In this notebook i have tried to give a visual overview of the data presented in the competition by means of various graphs in order to gain a better understanding of our objective in this competition."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('bmh')\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","min_scaler = MinMaxScaler()\n","scaler = StandardScaler()\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install jovian"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import jovian"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["INPUT_DIR = '../input/m5-forecasting-accuracy'\n","clndr = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\n","df_val = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\n","submsn = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n","prc = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\n","df = pd.read_csv(f'{INPUT_DIR}/sales_train_evaluation.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def sizeof_fmt(num, suffix='B'):\n","    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n","        if abs(num) < 1024.0:\n","            return \"%3.1f%s%s\" % (num, unit, suffix)\n","        num /= 1024.0\n","    return \"%.1f%s%s\" % (num, 'Yi', suffix)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df, verbose=True):\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2 \n","    print ('Initial Usage = {:5.2f} Mb'.format(start_mem))\n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                       df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)    \n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["reduce_mem_usage(df)\n","reduce_mem_usage(clndr)\n","reduce_mem_usage(prc)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clndr['date'] = pd.to_datetime(clndr.date)\n","clndr['days'] = clndr['date'].dt.day"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cln = clndr[:1941]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dates = cln['date']"]},{"cell_type":"markdown","metadata":{},"source":["### *Data Pre-processing*"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import time"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#da = clndr[0:1913]\n","da = clndr.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["da['event_name_1'] = da['event_name_1'].apply(lambda x: np.where(pd.isnull(x),0,1))\n","da['event_type_1'] = da['event_type_1'].apply(lambda x: np.where(pd.isnull(x),0,1))\n","da['event_name_2'] = da['event_name_2'].apply(lambda x: np.where(pd.isnull(x),0,1))\n","da['event_type_2'] = da['event_type_2'].apply(lambda x: np.where(pd.isnull(x),0,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["da = da.iloc[:,:14]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["da['date'] = pd.to_datetime(da.date)\n","da.set_index('date',inplace=True)\n","da.drop(['wm_yr_wk','weekday','d'],axis = 1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["l1 = ['wday','month','year']\n","for col in l1:\n","    da[col] = da[col].astype('category')\n","da = pd.get_dummies(da,columns=l1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def SMA(data):\n","    return data.mean()  \n","\n","def std(data):\n","    return data.std()\n","\n","def EMA(data):\n","    dats = data.astype(float)\n","    data['EMA'] = dats.ewm(span = 20).mean()\n","    return data['EMA']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cols = [i for i in df.columns if 'd_' in i ]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["date_tr = clndr['date']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_sale(item):\n","    tmp = df.set_index(df_val['id']).loc[item,cols]\n","    tmp = tmp.reset_index().drop('index',axis = 1).rename(columns = {0:item})\n","    return pd.merge(date_tr,tmp,left_index = True,right_index=True).set_index('date')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def dframe(item):\n","    tmp = da.copy()\n","    item_sale = get_sale(item)\n","    \n","    #tmp.replace('nan', np.nan).fillna(0)\n","    tmp = pd.merge(tmp,item_sale, left_index=True, right_index=True, how = 'left')\n","    tmp.rename(columns = {item:'item'},inplace =True)\n","    \n","    for i in (1,7,14,28,365):\n","        tmp['lag_'+str(i)] = tmp['item'].transform(lambda x: x.shift(i))\n","    \n","    \n","    for i in [7,14,28,60,180,365]:\n","        tmp['rolling_mean_'+str(i)] = tmp['item'].transform(lambda x: x.shift(28).rolling(i).mean())\n","        tmp['rolling_std_'+str(i)]  = tmp['item'].transform(lambda x: x.shift(28).rolling(i).std())\n","    \n","    \n","    tmp = tmp.replace('nan', np.nan).fillna(0)\n","    \n","    return tmp.to_numpy()\n","    \n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["### *Trying CNNs* "]},{"cell_type":"markdown","metadata":{},"source":["Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset,DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["scaler = StandardScaler()\n","y_scaler = MinMaxScaler()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def sliding_windows_mutli_features(data, seq_length):\n","    x = []\n","    y = []\n","    data_x = scaler.fit_transform(data)\n","    data_y = data[:,32].reshape(-1,1)\n","\n","    for i in range((data.shape[0])-seq_length-seq_length+1):\n","        #print (data.shape[0])\n","        #print (len(data)-seq_length-1)\n","        #print (i,(i+seq_length))\n","        _x = data_x[i:(i+seq_length),:] ## 16 columns for features  \n","        _y = data_y[i+seq_length:i+seq_length+seq_length] ## column 0 contains the labbel\n","        #print ('x - ',_x)\n","        #print ('y - ',_y)\n","        x.append(_x)\n","        y.append(_y)\n","\n","    return np.array(x),np.array(y).reshape(-1,28)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class FeatureDataset(Dataset):\n","    def __init__(self,feature,target):\n","        self.feature = feature\n","        self.target = target\n","    \n","    def __len__(self):\n","        return len(self.feature)\n","    \n","    def __getitem__(self,idx):\n","        item = self.feature.reshape(self.feature.shape[0],self.feature.shape[2],self.feature.shape[1])[idx]\n","        label = self.target[idx]\n","    \n","        \n","        return item,label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def pre_process(xdata,split,seq_length):\n","    trains = xdata[:split]\n","    tests = xdata[split:1913]\n","    val = xdata[1885:1941]\n","    evl = xdata[1913:]\n","    \n","    \n","    train_x,train_y = sliding_windows_mutli_features(trains, seq_length)\n","    test_x,test_y = sliding_windows_mutli_features(tests, seq_length)\n","    val_x,val_y = sliding_windows_mutli_features(val, seq_length=28)\n","    evl_x,evl_y = sliding_windows_mutli_features(evl, seq_length=28)\n","    \n","    train_set = FeatureDataset(train_x,train_y)\n","    test_set = FeatureDataset(test_x,test_y)\n","    val_set = FeatureDataset(val_x,val_y)\n","    evl_set = FeatureDataset(evl_x,evl_y)\n","    \n","    train_loader = DataLoader(dataset = train_set,\n","                         batch_size = 500\n","                         )\n","    test_loader = DataLoader(dataset = test_set,\n","                         batch_size = 300\n","                         )\n","    val_loader = DataLoader(dataset = val_set,\n","                         batch_size = 1\n","                         )\n","    evl_loader = DataLoader(dataset = evl_set,\n","                         batch_size = 1\n","                         )\n","    \n","    return train_loader,test_loader,val_loader,evl_loader"]},{"cell_type":"markdown","metadata":{},"source":["Building Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm import tqdm_notebook as tqdm\n","from torch.nn.utils import weight_norm\n","import gc\n","import collections\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CNN_Forecast(nn.Module):\n","    def __init__(self,c_in,c_bw,c_out,ks=3,d=2,s=1):\n","        super().__init__()\n","        self.conv1 = weight_norm(nn.Conv1d(c_in,c_bw,kernel_size=ks,dilation = d,padding = int((d*(ks-1))/2),stride = s))\n","        self.relu = nn.ReLU(inplace = True)\n","        self.conv2 = weight_norm(nn.Conv1d(c_bw,c_out,kernel_size=ks,dilation = d,padding = int((d*(ks-1))/2),stride = s))\n","        self.dp = nn.Dropout(0.2)\n","        self.shortcut = lambda x: x\n","        if c_in != c_out:\n","            self.shortcut = nn.Conv1d(c_in, c_out,kernel_size = 1,stride=1)\n","        \n","        y = torch.randn(c_in,28).view(-1,c_in,28)\n","        self.to_linear = None\n","        self.convs(y)\n","        \n","        self.fc1 = nn.Linear(self.to_linear,512)\n","        self.fc2 = nn.Linear(512,256)\n","        self.fc3 = nn.Linear(256,28)\n","        \n","    \n","    def convs(self,x):\n","        r = self.shortcut(x)\n","        x = self.relu(self.conv1(x))\n","        x = self.dp(x)\n","        x = F.leaky_relu(self.conv2(x),0.1)\n","        x = self.dp(x)\n","        #print ('r - ',r.shape)\n","        #print ('x - ',x.shape)\n","        r = r.view(x.shape)\n","        \n","        if self.to_linear is None:\n","            self.to_linear = x[0].shape[0]*x[0].shape[1]\n","        return x.add_(r)\n","        \n","    def forward(self,x):\n","        x = self.convs(x)\n","        x = x.view(-1,self.to_linear)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        \n","        return x\n","\n","model = CNN_Forecast(50,64,32)\n","print (model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def conv1d(c_in,c_out,stride= 1,dilation = 1,ks = 3):\n","    pad = int((dilation*(ks-1))/2)\n","    return nn.Conv1d(c_in,c_out,kernel_size = ks, stride = stride, dilation = dilation, padding = pad)\n","\n","def reg(max_ks):\n","    return nn.Sequential(nn.ReLU(inplace = True),\n","                         nn.Dropout(0.2),\n","                         nn.MaxPool1d(max_ks))\n","\n","\n","def size(s_len,ks,d,c_out,pool,max_ks=1,s=1):\n","    pad = pad = int((d*(ks-1))/2)\n","    l_out = (s_len+2*pad-d*(ks-1)-1)/s + 1\n","    if not pool:\n","        size = c_out*l_out\n","    else:\n","        l_out = (l_out-(max_ks-1)-1)/max_ks + 1\n","        size = c_out*l_out\n","    \n","    return int(size)\n","\n","    \n","class ConvNet(nn.Module):\n","    def __init__(self,n_start,c_out,ks,d,max_ks):\n","        super().__init__()\n","        self.conv0 = conv1d(n_start,c_out[0],ks = ks[0],dilation = d[0])\n","        self.conv1 = conv1d(n_start,c_out[1],ks = ks[1],dilation = d[1])\n","        self.conv2 = conv1d(n_start,c_out[2],ks = ks[2],dilation = d[2])\n","        \n","        self.reg0 = reg(max_ks)\n","        self.reg1 = reg(max_ks)\n","        self.reg2 = reg(max_ks)\n","        \n","        self.size0 = size(28,ks[0],d[0],c_out[0],max_ks = max_ks,pool = True)\n","        self.size1 = size(28,ks[1],d[1],c_out[1],max_ks = max_ks,pool = True)\n","        self.size2 = size(28,ks[2],d[2],c_out[2],max_ks = max_ks,pool = True)\n","        \n","        self.fc1 = nn.Linear((self.size0+self.size1+self.size2),100)\n","        self.fc2 = nn.Linear(100,1)\n","\n","    def forward(self,x):\n","        x0 = self.reg0(self.conv0(x))\n","        x1 = self.reg1(self.conv1(x))\n","        x2 = self.reg2(self.conv2(x))\n","        \n","        x0 = x0.view(-1,self.size0)\n","        x1 = x1.view(-1,self.size1)\n","        x2 = x2.view(-1,self.size2)\n","        \n","        x = torch.cat([x0,x1,x2],1)\n","        \n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        \n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Trainings1(product):\n","    running_loss = .0\n","    model.train()\n","    \n","    for idx, (inputs,labels) in enumerate(train_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device,non_blocking = True)\n","        optimizer.zero_grad()\n","        preds = model(inputs)\n","        loss = criterion(preds,labels)\n","        loss.backward()\n","        #scheduler.step(loss)\n","        optimizer.step()\n","        running_loss += loss\n","        \n","        #print (f\"epoch {epoch}, datapoints {idx*10}\")\n","    \n","    train_loss = running_loss/len(train_loader)\n","    train_losses[str(product)].append(train_loss.cpu().detach().numpy().item())\n","        \n","        \n","def Validations1(product):\n","    running_loss = .0\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(test_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device,non_blocking = True)\n","            optimizer.zero_grad()\n","            preds = model(inputs)\n","            loss = criterion(preds,labels)\n","            #scheduler.step(loss)\n","            running_loss += loss\n","            #print (f\"epoch {epoch}, datapoints {idx*10}\")\n","            \n","        valid_loss = running_loss/len(test_loader)\n","        valid_losses[str(product)].append(valid_loss.cpu().detach().numpy().item())\n","            \n","def Predicts1(product):\n","    with torch.no_grad():\n","            for idx, (inputs,_) in enumerate(val_loader):\n","                inputs = inputs.to(device)\n","                optimizer.zero_grad()\n","                preds = model(inputs)\n","                pred_dict.update({product:preds.cpu().detach().numpy().reshape(28,).tolist()})\n","\n","            \n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_cv_loss = pd.DataFrame()\n","valid_cv_loss = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def Trainings1(product):\n","    model.train()\n","    \n","    for idx, (inputs,labels) in enumerate(train_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        preds = model(inputs)\n","        loss = criterion(preds,labels)\n","        loss.backward()\n","        #scheduler.step(loss)\n","        optimizer.step()\n","        \n","        \n","def Validations1(product):\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(test_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            preds = model(inputs)\n","            loss = criterion(preds,labels)\n","            #scheduler.step(loss)\n","\n","            \n","def Predicts1(product):\n","    with torch.no_grad():\n","            for idx, (inputs,_) in enumerate(val_loader):\n","                inputs = inputs.to(device)\n","                optimizer.zero_grad()\n","                preds = model(inputs)\n","                pred_dict.update({product:preds.cpu().detach().numpy().reshape(28,).tolist()})\n","\n","            \n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["start1 = time.time()\n","#start = time.time()\n","seq_length = 28\n","split = 1500\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","criterion = nn.MSELoss()\n","epochs = 20\n","\n","prediction = [0]*28\n","pred_dict = {}\n","\n","first = True\n","\n","model = CNN_Forecast(50,64,32,ks = 11,d=3).double()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=500,factor =0.5 ,min_lr=1e-7, eps=1e-08)\n","\n","idir_torch2 = '../input/torch-model-new'\n","loaded = torch.load(f'{idir_torch2}/cpoints_new.pth')\n","\n","model.load_state_dict(loaded['model_state'])\n","optimizer.load_state_dict(loaded['optim_state'])\n","model.to(device)\n","for state in optimizer.state.values():\n","    for k, v in state.items():\n","         if isinstance(v, torch.Tensor):\n","            state[k] = v.cuda()\n","            \n","for product in tqdm(df_val['id']):\n","    \n","    \n","    dset = dframe(product)\n","    #dset = np.load('.'.join([product,'npy']))\n","    train_loader,test_loader,val_loader,evl_loader = pre_process(dset,split,seq_length)\n","    \n","    '''for epoch in range(epochs):\n","        Trainings1(product)\n","        Validations1(product)'''\n","   \n","    Predicts1(product)\n","\n","pred_d = pd.DataFrame(pred_dict).T\n","pred_d.to_csv('predictions1.csv')    \n","    \n","end = time.time()\n","\n","print('Time taken is {}s'.format(end-start1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pd.DataFrame(pred_dict).T"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["checkpoint = {\n","        \"model_state\":model.state_dict(),\n","        \"optim_state\":optimizer.state_dict()\n","    }\n","    \n","torch.save(checkpoint,'cpoints_new.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pred_d.to_csv('predictions_val.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["pred_d"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["idir_dset = '../input/predictions'\n","loaded_dset1 = pd.read_csv(f'{idir_dset}/predictions1.csv')\n","\n","idir_dset2 = '../input/predictions2'\n","loaded_dset2 = pd.read_csv(f'{idir_dset2}/predictions2.csv')\n","\n","idir_dset3 = '../input/predictions-evl'\n","loaded_dset3 = pd.read_csv(f'{idir_dset3}/predictions_evl.csv')\n","\n","idir_dset4 = '../input/predictions-val'\n","loaded_dset4 = pd.read_csv(f'{idir_dset4}/predictions_val.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds = pd.concat([loaded_dset4,loaded_dset3])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds.columns = submsn.columns\n","preds.index = submsn.index"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submsn"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds.set_index('id').to_csv('submission4.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["idir_sb = '../input/m5-first-public-notebook-under-0-50'\n","submt_dset2 = pd.read_csv(f'{idir_sb}/submission_1.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submt_dset2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds[preds['F25']<0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
